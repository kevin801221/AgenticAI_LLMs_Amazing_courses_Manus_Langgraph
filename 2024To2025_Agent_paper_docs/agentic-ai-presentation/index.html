<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/black.css">

    <title>Agentic AI: The Next Frontier</title>

    <style>
        .reveal {
            --r-link-color: #2AAA7F;
            --r-link-color-hover: #1f8862;
            --r-selection-background-color: #2AAA7F;
        }

        h1,
        h2,
        h3 {
            margin-bottom: 0.5em;
        }

        ul {
            margin-left: 20px;
        }

        .logo {
            height: 40px;
            max-width: 200px;
            vertical-align: middle;
            margin-right: 10px;
            background-color: white;
            border-radius: 10px;
            padding: 5px;
        }

        .author-info {
            position: fixed;
            bottom: 10px;
            left: 10px;
            font-size: 18px;
            color: #888;
            z-index: 1000;
        }

        img {
            border-radius: 10px;
        }

        .highlighted {
            color: var(--r-link-color);
        }

        mark {
            background-color: var(--r-link-color);
            color: white;
        }

        tbody {
            font-size: 20px;
        }

        html {
            text-wrap: balance;
        }
    </style>
</head>

<body>

    <div class="reveal">
        <div class="author-info">
            Gianlorenzo Occhipinti<br>
            üçª Renuo Beer Talks 22.01.2025
        </div>

        <div class="slides">

            <!-- Slide 1: AI Trends 2025 -->
            <section>
                <h2>AI Trends 2025 üöÄ</h2>
            </section>

            <!-- Slide 2: Key Findings from LangChain Report -->
            <section>
                <h4>ü¶ú LangChain's State of AI Agents Report</h4>
                <img src="img/langchain_state_of_ai_agents.png" alt="LangChain Logo">
                <p style="font-size: 12px; margin-top: 10px;"><a
                        href="https://www.langchain.com/stateofaiagents">Source: LangChain State of AI Agents Report
                        (2024)</a></p>
                <aside class="notes">
                    <ul>
                        <li>Surveyed 1,300 professionals</li>
                        <li>December 2024</li>
                    </ul>
                </aside>
            </section>
            <section data-auto-animate>
                <h4>ü¶ú LangChain's State of AI Agents Report</h4>
                <img src="img/langchain_state_of_ai_agents_chart_1_2.png" alt="LangChain Chart 1 / 2"
                    style="width: 400px; height: 400px; object-fit: cover;object-position: 0 0;">
                <p style="font-size: 12px; margin-top: 10px;"><a
                        href="https://www.langchain.com/stateofaiagents">Source: LangChain State of AI Agents Report
                        (2024)</a></p>
            </section>
            <section data-auto-animate>
                <h4>ü¶ú LangChain's State of AI Agents Report</h4>
                <img src="img/langchain_state_of_ai_agents_chart_1_2.png" alt="LangChain Chart 1 / 2"
                    style="width: 100%;">
                <p style="font-size: 12px; margin-top: 10px;"><a
                        href="https://www.langchain.com/stateofaiagents">Source: LangChain State of AI Agents Report
                        (2024)</a></p>
            </section>
            <section>
                <h4>ü¶ú LangChain's State of AI Agents Report</h4>
                <ul>
                    <li class="fragment"><strong class="highlighted">51%</strong> of surveyed companies have integrated
                        AI agents into
                        production
                        environments.</li>
                    <li class="fragment"><strong class="highlighted">78%</strong> of organizations are actively
                        developing AI agents for
                        near-future
                        deployment.</li>
                    <li class="fragment"><strong class="highlighted">90%</strong> of non-tech companies are either using
                        or planning to use
                        AI agents.</li>
                </ul>

                <p style="font-size: 12px; margin-top: 50px;"><a
                        href="https://www.langchain.com/stateofaiagents">Source: LangChain State of AI Agents Report
                        (2024)</a></p>

            </section>

            <!-- Slide 3: Introducing the Next Buzzword of 2025 -->
            <section>
                <h2>The Next Buzzword of 2025</h2>
            </section>

            <!-- Slide 4: Agentic AI -->
            <section>
                <h1 style="font-size: 4em; text-align: center;">Agentic AI</h1>
                <blockquote class="fragment">
                    The future of autonomous, intelligent
                    systems.
                </blockquote>
            </section>

            <!-- Slide 5: Media Mentions of Agentic AI -->
            <section>
                <h2>Media Mentions of Agentic AI</h2>
                <ul>
                    <li class="fragment">
                        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Forbes_logo.svg/1024px-Forbes_logo.svg.png"
                            alt="Forbes Logo" class="logo">
                        <em><a href="https://www.forbes.com/councils/forbestechcouncil/2025/01/21/looking-at-the-crystal-ball-2025-predictions-for-agentic-ai/?utm_source=chatgpt.com"
                                target="_blank">Looking At The Crystal Ball: 2025 Predictions For Agentic AI</a></em>
                        <span style="font-size: 30px;">(21.01.2025)</span>
                    </li>
                    <li class="fragment">
                        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Time_Magazine_logo.svg/596px-Time_Magazine_logo.svg.png"
                            alt="TIME Logo" class="logo">
                        <em><a href="https://time.com/7204665/ai-predictions-2025/" target="_blank">5 Predictions for AI
                                in 2025</a></em> <span style="font-size: 30px;">(16.01.2025)</span>
                    </li>
                    <li class="fragment">
                        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Wired_logo.svg/1024px-Wired_logo.svg.png"
                            alt="Wired Logo" class="logo">
                        <em><a href="https://www.wired.com/story/the-prompt-ai-agents-how-much-should-we-let-them-do"
                                target="_blank">AI Agents Are Here. How Much Should We Let Them Do?</a></em> <span
                            style="font-size: 30px;">(15.01.2025)</span>
                    </li>
                </ul>
            </section>

            <section>
                <h2>üéñÔ∏è Honorable Mention üéñÔ∏è</h2>
                <iframe style="border-radius: 10px;" width="1280" height="520"
                    src="https://www.youtube.com/embed/afWBnxWQDKk" title="Transform Your Business With Agentic AI"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </section>

            <!-- Slide 6: What Is Agentic AI? -->
            <section data-auto-animate>
                <h2>Agentic AI</h2>
                <q class="fragment">AI that autonomously make decisions, take actions, and learn to achieve specific
                    goals.</q>

                <aside class="notes">
                    <ul>
                        <li>Agentic AI: Autonomous AI system</li>
                        <li>Key capabilities: Decision-making, action-taking, self-learning</li>
                        <li>Purpose: Achieves specific goals independently</li>
                        <li>Behavior: Like an intelligent virtual assistant</li>
                        <li>Process: Operates through 4 distinct stages</li>
                    </ul>
                </aside>
                <p>
                    <img class="fragment" src="img/agentic_ai.png"
                        alt="Agentic AI Capabilities: Perceive, Reason, Act, Learn"
                        style="width: 60%; margin: 20px auto;">
                </p>



                <p style="font-size: 12px; margin-top: 50px;"><a
                        href="https://medium.com/@elisowski/ai-agents-vs-agentic-ai-whats-the-difference-and-why-does-it-matter-03159ee8c2b4">Source:
                        AI Agents vs Agentic AI, What's the Difference and Why Does It Matter?</a></p>
            </section>

            <section data-auto-animate>
                <h2>Agentic AI vs AI Agents</h2>
                <img src="img/agentic_ai_vs_ai_agents.png" alt="Agentic AI vs AI Agents">
            </section>


            <section data-auto-animate>
                <h2>Agentic AI vs AI Agents</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Aspect</th>
                            <th>Agentic AI</th>
                            <th>AI Agent</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td class="fragment" data-fragment-index="1"><strong class="highlighted">Autonomy
                                    Level</strong></td>
                            <td class="fragment" data-fragment-index="2">Highly autonomous</td>
                            <td class="fragment" data-fragment-index="2">Limited autonomy</td>
                        </tr>
                        <tr>
                            <td class="fragment" data-fragment-index="3"><strong
                                    class="highlighted">Goal-Orientation</strong></td>
                            <td class="fragment" data-fragment-index="4">Goal-driven</td>
                            <td class="fragment" data-fragment-index="4">Task-specific</td>
                        </tr>
                        <tr>
                            <td class="fragment" data-fragment-index="5"><strong class="highlighted">Learning
                                    Capabilities</strong></td>
                            <td class="fragment" data-fragment-index="6">Continuously learns and improves</td>
                            <td class="fragment" data-fragment-index="6">May not learn or only learns within set rules
                            </td>
                        </tr>
                        <tr>
                            <td class="fragment" data-fragment-index="7"><strong class="highlighted">Complexity</strong>
                            </td>
                            <td class="fragment" data-fragment-index="8">Handles complex, dynamic environments</td>
                            <td class="fragment" data-fragment-index="8">Handles simpler, more structured tasks</td>
                        </tr>
                        <tr>
                            <td class="fragment" data-fragment-index="9"><strong class="highlighted">Decision-Making
                                    Process</strong></td>
                            <td class="fragment" data-fragment-index="10">Makes decisions based on reasoning and
                                analysis</td>
                            <td class="fragment" data-fragment-index="10">Pre-programmed responses to inputs</td>
                        </tr>
                        <tr>
                            <td class="fragment" data-fragment-index="11"><strong class="highlighted">Interaction with
                                    Environment</strong></td>
                            <td class="fragment" data-fragment-index="12">Actively adapts to surroundings and changes
                            </td>
                            <td class="fragment" data-fragment-index="12">Reacts to set inputs but doesn't adapt</td>
                        </tr>
                        <tr>
                            <td class="fragment" data-fragment-index="13"><strong class="highlighted">Responsiveness to
                                    Change</strong></td>
                            <td class="fragment" data-fragment-index="14">Changes its goals and methods autonomously
                            </td>
                            <td class="fragment" data-fragment-index="14">Limited ability to adapt to new situations
                            </td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section data-auto-animate data-state="stats">
                <h2>Agentic AI</h2>
                <img class="fragment" src="img/agentic_ai.png"
                    alt="Agentic AI Capabilities: Perceive, Reason, Act, Learn">
                <div class="fragment" style="    
                position: absolute;
                bottom: 112px;
                left: 130px;
                width: 210px;
                height: 100px;
                border: 10px solid red;
                border-radius: 10px;
            ">

                </div>
            </section>

            <!-- Slide 7: Agentic AI Actions -->

            <section data-auto-animate>
                <h2>Agentic AI Actions</h2>
            </section>

            <section data-auto-animate>
                <h2>Agentic AI Actions</h2>

                <img class="fragment" src="img/devin_ai.png" alt="Devin AI" style="max-width: 700px">
                <p style="font-size: 12px; margin-top: 50px;"><a href="https://devin.ai/">Source:
                        Devin AI</a></p>
            </section>



            <!-- Slide 8: Tools for Agents -->
            <section data-auto-animate>
                <h2>How can we build Agentic AI</h2>
                <ul>
                    <li class="fragment"><strong class="highlighted">Model (LLM)</strong>: The "brain" generating
                        text/logic.</li>
                    <li class="fragment"><strong class="highlighted">Orchestration</strong>: Multi-step logic & planning
                        <ul>
                            <li class="fragment"><strong class="">ReAct</strong> (<a
                                    href="https://arxiv.org/pdf/2210.03629">ReAct</a>)
                                <aside class="notes">
                                    Prompt engineering framework that provides a "thought" process strategy for
                                    language models to Reason and take action on a user query, with or without
                                    in-context
                                    examples
                                </aside>
                            </li>
                            <li class="fragment"><strong class="">Chain-of-Thought</strong> (<a
                                    href="https://arxiv.org/pdf/2201.11903">CoT</a>)
                                <aside class="notes">
                                    Prompt engineering framework that enables reasoning
                                    capabilities through intermediate steps.
                                </aside>
                            </li>
                            <li class="fragment"><strong class="">Tree-of-Thoughts</strong> (<a
                                    href="https://arxiv.org/pdf/2305.10601"> ToT</a>)
                                <aside class="notes">
                                    Prompt engineering framework that is well suited for
                                    exploration or strategic lookahead tasks. It generalizes over chain-of-thought
                                    prompting.
                                </aside>
                            </li>
                        </ul>
                    </li>
                    <li class="fragment"><strong class="highlighted">Tools</strong>:
                        <aside class="notes">
                            The keys to the outside world.
                            A language model is only as good as what it has learned from its training data. But
                            regardless of how much
                            data we throw at a model, they still lack the fundamental ability to interact with the
                            outside
                            world. So how can we empower our models to have real-time, context-aware interaction with
                            external systems?
                        </aside>
                        <ul>
                            <li class="fragment"><strong class="highlighted"> Extensions </strong> (agent-side,
                                real-time external calls)</li>
                            <li class="fragment"><strong class="highlighted"> Functions </strong> (model outputs
                                arguments, executed client-side)</li>
                            <li class="fragment"><strong class="highlighted"> Data Stores </strong> (vector DB, document
                                retrieval)</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section data-auto-animate>
                <h2>How can we build Agentic AI</h2>
                <img src="img/agents_whitepaper.png" alt="Agentic AI Tools"
                    style="height: 500px; border: 2px solid #333; box-shadow: 5px 5px 15px rgba(0,0,0,0.3); background: #fff; transform: perspective(1000px) rotateY(5deg);">
                <p style="font-size: 12px; margin-top: 20px;"><a href="https://www.kaggle.com/whitepaper-agents">Source:
                        Agents</a></p>
            </section>


            <!-- Slide 9: Extensions -->
            <section data-auto-animate>
                <h2>Extensions</h2>
            </section>
            <section data-auto-animate>
                <h2>Extensions</h2>
                <p>Bridging between the model and the outside world</p>
                <img src="img/extensions.png" alt="Extensions">
                <ul>
                    <li class="fragment"> Teaches the agent how to use the API endpoint</li>
                    <li class="fragment"> Teaches the agent what arguments are needed</li>
                </ul>
            </section>

            <section data-auto-animate>
                <h2>Extensions</h2>
                <p>Modular tools that give agents real-world capabilities</p>
                <aside class="notes">
                    Extensions are modular tools configured with the agent. The agent dynamically
                    chooses appropriate Extensions based on built-in examples and the user's needs at runtime.
                </aside>
                <img src="img/extentions_relationship.png" alt="Function">
            </section>

            <section data-auto-animate>
                <h2>Example</h2>
                <pre><code data-trim data-noescape data-line-numbers="3-6|8-24|25-26">
import vertexai
import pprint

PROJECT_ID = "YOUR_PROJECT_ID" 
REGION = "us-central1"
vertexai.init(project=PROJECT_ID, location=REGION)

zefix_extension = Extension.create(
    manifest={
            "name": "zefix",
            "description": "Access to the swiss central 
                business name index",
            "api_spec": {
                "openApiYaml": ZEFIX_OPENAPI,
            },
            "auth_config": {
                "auth_type": "HTTP_BASIC_AUTH",
                "httpBasicAuthConfig": {
                  "credentialSecret": ZEFIX_SECRET,
                },
            },
        }
)

response = zefix_extension.query("Search for Renuo")
print(response.steps[-1].parts[-1].text)
                </code></pre>
                <aside class="notes">
                    This example shows how to create an Extension that interfaces with the Swiss Business Index API.
                    The Extension is configured with authentication and API specifications, then used to make queries.
                </aside>
            </section>

            <section>
                <h1> üòµ‚Äçüí´ DEMO</h1>
                <p style="font-size: 12px;"><a href="zefix_extension_demo.ipynb">Link: Zefix Extension Notebook</a></p>
            </section>

            <!-- Slide 10: Functions -->
            <section data-auto-animate>
                <h2>Functions</h2>
            </section>

            <section data-auto-animate>
                <h2>Functions</h2>
                <p>Modules that accomplish a specific task</p>
                <img src="img/functions.png" alt="Extensions">
                <ul>
                    <li class="fragment"> Model decides which functions to use and when</li>
                    <li class="fragment"> Executed on the <span class="highlighted">client side</span></li>
                </ul>
            </section>

            <section data-auto-animate>
                <h2>Functions</h2>
                <p>API endpoint execution is handled by the client</p>
                <aside class="notes">
                    Functions offload API endpoint execution to client-side, giving developers more control.
                    Benefits include granular data flow management and reduced external dependencies. Though
                    architectural
                    differences are subtle, the added control makes Functions appealing for developers.
                </aside>
                <img src="img/functions_vs_extensions.png" alt="Functions vs Extensions">
            </section>

            <section data-auto-animate>
                <h2>Functions</h2>
                <aside class="notes">
                    There are many reasons why a Developer might choose to use functions over Extensions, but a few
                    common use cases are:
                </aside>
                <ul>
                    <li class="fragment"> <strong class="highlighted">API Layer Control</strong>: Execute API calls
                        through middleware or frontend frameworks</li>
                    <li class="fragment"> <strong class="highlighted">Security & Auth</strong>: Handle restricted APIs
                        not accessible to agent infrastructure</li>
                    <li class="fragment"> <strong class="highlighted">Operation Timing</strong>: Manage batch operations
                        and human-in-loop review flows</li>
                    <li class="fragment"> <strong class="highlighted">Data Transformation</strong>: Apply additional
                        processing to API responses</li>
                    <li class="fragment"> <strong class="highlighted">Rapid Development</strong>: Iterate on agent logic
                        without deploying API infrastructure</li>
                </ul>
            </section>

            <section data-auto-animate>
                <h2>Example</h2>
                <pre><code data-trim data-noescape data-line-numbers="4-10 | 13-17">
from vertexai.preview.extensions import Extension

def list_extensions():
    """Provides a list of Vertex AI extensions available
     to the user.

    Returns:
        list[tuple[str, str, str]]: A list of tuples where 
        each tuple contains (in order) the display name, 
        the resource name, and the creation time of an extension.
    """
    extensions_list = Extension.list()
    return [(
        e.display_name, 
        e.resource_name, 
        e.create_time.strftime("%m/%d/%Y, %H:%M:%S")
        ) for e in Extension.list()]
                </code></pre>
            </section>

            <section data-auto-animate>
                <h2>Example</h2>
                <pre><code data-trim data-noescape data-line-numbers="1 | 3-8 | 10-12 | 14-18 | 17 | 21 | 22-23 | 25 | 41">
model = GenerativeModel("gemini-1.5-flash-001")

user_prompt_content = Content(
    role="user",
    parts=[
        Part.from_text("Which Vetex AI extension do I have access to ?"),
    ],
)

support_tool = Tool(
    function_declarations=[list_extensions_function],
)

response = model.generate_content(
    user_prompt_content,
    generation_config=GenerationConfig(temperature=0),
    tools=[support_tool],
)


for function_call in response.candidates[0].function_calls:
    if function_call.name == "list_extensions":
        api_response = list_extensions()

    response = model.generate_content(
        [
            user_prompt_content,  
            response.candidates[0].content,  
            Content(
                parts=[
                    Part.from_function_response(
                        name=function_call.name,
                        response={"content": api_response},
                    ),
                ],
            ),
        ],
        tools=[support_tool],
    )

    print(response.text)
                </code></pre>
            </section>

            <section>
                <h1> üòµ DEMO</h1>
                <p style="font-size: 12px;"><a href="function_calling_demo.ipynb">Link: Function Calling Notebook</a>
                </p>
            </section>

            <!-- Slide 11: Data Stores -->
            <section data-auto-animate>
                <h2>Data Stores</h2>
            </section>
            <section data-auto-animate>
                <h2>Data Stores</h2>
                <p>Connecting agents to real-time data sources</p>
                <img src="img/data_stores.png" alt="Data Stores" style="width: 600px">
                <ul>
                    <li class="fragment"> Typically implemented as a <span class="highlighted">vector database</span>
                    </li>
                    <li class="fragment"> <span class="highlighted">RAG</span> (Retrieval Augmented Generation) is a
                        common use case</li>
                </ul>
            </section>

            <section data-auto-animate>
                <h2>Data Stores</h2>
                <aside class="notes">
                    1. A user query is sent to an embedding model to generate embeddings for the query
                    2. The query embeddings are then matched against the contents of the vector database using a
                    matching algorithm like SCaNN
                    3. The matched content is retrieved from the vector database in text format and sent back to
                    the agent
                    4. The agent receives both the user query and retrieved content, then formulates a response
                    or action
                    5. A final response is sent to the user
                </aside>
                <img src="img/data_stores_lifecycle.png" alt="Data Stores Lifecycle">
            </section>

            <!-- Slide 12: Tools Recap -->
            <section data-auto-animate>
                <h2>Tools Recap</h2>
                <table>
                    <thead>
                        <tr>
                            <th></th>
                            <th class="fragment">Extensions</th>
                            <th class="fragment">Function</th>
                            <th class="fragment">Data Stores</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td> <span class="highlighted">Execution</span></td>
                            <td class="fragment">Agent-side</td>
                            <td class="fragment">Client-side</td>
                            <td class="fragment">Agent-side</td>
                        </tr>
                        <tr>
                            <td style="vertical-align: middle;"> <span class="highlighted">Use Case</span></td>
                            <td class="fragment" style="vertical-align: middle;">
                                <ul style="margin-left: 0; font-size: 30px; margin-bottom: 10px;">
                                    <li style="margin-bottom: 8px;">Direct API control by agent</li>
                                    <li style="margin-bottom: 8px;">Pre-built Extension usage</li>
                                    <li>Multi-step API workflows</li>
                                </ul>
                            </td>
                            <td class="fragment" style="vertical-align: middle;">
                                <ul style="margin-left: 0; font-size: 30px; margin-bottom: 10px;">
                                    <li style="margin-bottom: 8px;">Restricted API access</li>
                                    <li style="margin-bottom: 8px;">Complex timing requirements</li>
                                    <li>Internal/private APIs</li>
                                </ul>
                            </td>
                            <td class="fragment" style="vertical-align: middle;">
                                <ul style="margin-left: 0; font-size: 30px; margin-bottom: 10px;">
                                    <li style="margin-bottom: 8px;">RAG implementation</li>
                                    <li style="margin-bottom: 8px;">Multi-format data handling</li>
                                    <li>Semantic search needs</li>
                                </ul>
                            </td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <!-- Slide 9: Summary & Closing -->
            <section data-auto-animate>
                <h2 id="title">Summary & Closing</h2>
                <ul>
                    <li class="fragment"><strong class="highlighted">Agentic AI</strong>: Multi-step, self-improving AI
                        that goes beyond chat.</li>
                    <li class="fragment"><strong class="highlighted">Core building blocks</strong>: Model,
                        Orchestration, Tools, & Learning Loop.</li>
                    <li class="fragment"><strong class="highlighted">Practical Use Cases</strong>: Customer service, dev
                        ops, marketing, healthcare, etc.
                    </li>
                </ul>
            </section>

            <section data-auto-animate>
                <h2 id="title">Summary & Closing</h2>

                <h2>Questions? ü§î</h2>
            </section>

        </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <link rel="stylesheet" href="plugin/highlight/monokai.css" />
    <script src="plugin/highlight/highlight.js"></script>
    <script>
        Reveal.initialize({
            hash: true,

            transition: 'slide',
            plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
        });
    </script>


</body>

</html>
