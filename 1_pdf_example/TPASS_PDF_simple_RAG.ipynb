{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf sentence_transformers qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 中部地區公共運輸定期票 售票與使用須知  \n",
      "2023年6月15日起適用  \n",
      "壹、  發行依據  \n",
      "依據「交通部公路總局執行公共運輸通勤月票補助作業要點」，由 臺\n",
      "中市政府、彰化縣政府、南投縣政府及苗栗縣政府 (以下簡稱地方政府 )委託\n",
      "臺中捷運股份有限公司 (以下簡稱台中捷運公司 )發行「臺中市定期票」 、 「彰\n",
      "化縣定期票」、「南投縣定期票」及「中彰投苗定期票」 。 \n",
      "貳、  發售票種及說明  \n",
      "中部地區 公共運輸 各方案定期票售價及可搭乘之運具範圍如表一及表\n",
      "二，持卡人 於定期票有效期間內，不限里程、不限次數搭乘 定期票所屬之\n",
      "公共運輸運具，適用範圍如下。  \n",
      "表一 定期票售票方案1 \n",
      "區域方案  購買對象  售價 (元) 可使用運具  \n",
      "臺中市  \n",
      "定期票  臺中市民  299 臺中市境內臺鐵、捷運 、市\n",
      "區公車、公共自行車  非臺中市民  599 \n",
      "彰化縣  \n",
      "定期票  民眾  699 彰化縣境內之臺鐵2、公路客\n",
      "運、市區公車、公共自行車  \n",
      "南投縣  \n",
      "定期票  民眾  699 南投縣境內之臺鐵2(集集\n",
      "線)、公路客運、市區公車  \n",
      "中彰投苗  \n",
      "定期票  臺中市民  699 中彰投苗4縣市境內之臺\n",
      "鐵、捷運、市區公車 、公共\n",
      "自行車、公路客運  非臺中市民  999 \n",
      "註1：各票種之適用範圍及路線以中彰投苗 4縣市政府及臺中捷運公司之公告為準。  \n",
      "註2：購買彰化縣、南投縣定期票，初期尚無法使用臺鐵，俟完成整合後方可使用，並\n",
      "將另行公告。\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "import re  \n",
    "\n",
    "# 將一份 PDF 切成多個 chunks \n",
    "# 用 \\n 分割, 每個 chunk size 上限為 256\n",
    "def pdf_to_chunk(path, start_page=1, end_page=None): \n",
    "    # 使用 PyPDFLoader 來加載 PDF 文件\n",
    "    loader = PyPDFLoader(path) \n",
    "\n",
    "    # 將 PDF 文件的每一頁作為單獨的對象加載到 pages 列表中\n",
    "    pages = loader.load() \n",
    "\n",
    "    # 計算 PDF 文件的總頁數\n",
    "    total_pages = len(pages) \n",
    "    \n",
    "    # 創建一個 CharacterTextSplitter 對象\n",
    "    # 用來將文本分割成多個塊\n",
    "    text_splitter = CharacterTextSplitter( \n",
    "        separator=\"\\\\n\",  # 使用 \"\\n\" 作為分隔符\n",
    "        chunk_size=256,  # 每個塊的最大字符數為 256\n",
    "        chunk_overlap=20  # 塊之間有 20 個字符的重疊\n",
    "    ) \n",
    "\n",
    "    # 如果沒有指定結束頁面，則默認處理到文檔的最後一頁\n",
    "    if end_page is None: \n",
    "        end_page = len(pages) \n",
    "\n",
    "    # 用來存儲所有塊的列表\n",
    "    lst_text = [] \n",
    "    \n",
    "    # 循環處理指定頁面範圍內的每一頁\n",
    "    for i in range(start_page-1, end_page): \n",
    "        # 將當前頁面的文本分割成多個塊\n",
    "        chuncks = text_splitter.split_text(pages[i].page_content) \n",
    "\n",
    "        # 遍歷每一個塊\n",
    "        for chunk in chuncks: \n",
    "            # 使用正則表達式去除多餘的空白字符\n",
    "            text = re.sub(r'\\\\s+', ' ', chunk) \n",
    "\n",
    "            # 將處理後的文本塊添加到列表中\n",
    "            lst_text.append(text) \n",
    "    \n",
    "    # 返回包含所有文本塊的列表\n",
    "    return lst_text  \n",
    "\n",
    "# 調用 pdf_to_chunk 函數，將一個 PDF 文件的內容分割為多個文本塊\n",
    "lst_chunk = pdf_to_chunk('TPASS.pdf')  # 替換 'your_file.pdf' 為實際的 PDF 文件路徑\n",
    "\n",
    "# 打印第一個文本塊以驗證結果\n",
    "print(lst_chunk[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer  \n",
    "\n",
    "# 將文本塊轉換為向量表示\n",
    "def chunk_to_vector(chunks): \n",
    "    # 初始化使用的預訓練模型\n",
    "    model = SentenceTransformer('all-MiniLM-L12-v2') \n",
    "\n",
    "    # 將文本塊列表轉換為向量表示\n",
    "    arr_vector = model.encode(chunks) \n",
    "    \n",
    "    # 返回向量列表\n",
    "    return arr_vector \n",
    "\n",
    "# 調用 chunk_to_vector 函數，將文本塊列表轉換為向量表示\n",
    "# arr_vectors 的形狀為 (n, 384)，其中 n 是文本塊的數量，384 是向量的維度\n",
    "arr_vectors = chunk_to_vector(lst_chunk) \n",
    "\n",
    "# 打印向量數組的形狀以驗證結果\n",
    "print(arr_vectors.shape)  # 例如輸出: (57, 384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient \n",
    "# from qdrant_client.http import models \n",
    "# from qdrant_client.http.models import PointStruct  \n",
    "\n",
    "# # 連接到 Qdrant 並創建一個集合\n",
    "# def connection(v_dim, collection): \n",
    "#     # v_dim: 向量維度, collection: 類似於資料庫中的表名稱\n",
    "\n",
    "#     # 創建一個 Qdrant 客戶端，連接到本地 Qdrant 伺服器\n",
    "#     client = QdrantClient(\"http://localhost:6333\")  \n",
    "    \n",
    "#     # 重新創建一個集合（如果存在則刪除並重新創建）\n",
    "#     client.recreate_collection(         \n",
    "#         collection_name=collection,  # 集合名稱，相當於資料庫中的表名稱\n",
    "#         vectors_config=models.VectorParams(  \n",
    "#             distance=models.Distance.COSINE,  # 使用余弦距離作為向量相似度度量\n",
    "#             size=v_dim  # 向量的維度，例如 384\n",
    "#         ),         \n",
    "#         optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),  # 優化器配置，用於優化存儲和查詢性能\n",
    "#         hnsw_config=models.HnswConfigDiff(on_disk=True, m=16, ef_construct=100)  # HNSW 配置，用於加速向量搜索\n",
    "#     )     \n",
    "\n",
    "#     # 返回 Qdrant 客戶端對象，用於後續的操作\n",
    "#     return client  \n",
    "\n",
    "# # 將向量及其對應的數據插入到 Qdrant 集合中\n",
    "# def upsert_vector(client, collection, vectors, data): \n",
    "#     # vectors: 向量數組 (Array)\n",
    "#     # data: 與向量對應的數據列表 (List of dictionaries)，如 [{\"text\": \"example text\"}]\n",
    "\n",
    "#     # 遍歷所有的向量及其對應的數據\n",
    "#     for i, vector in enumerate(vectors):         \n",
    "#         client.upsert(  # 插入或更新向量和數據             \n",
    "#             collection_name=collection,  # 指定要插入的集合\n",
    "#             points=[PointStruct(  # 定義要插入的數據點\n",
    "#                 id=i,  # 每個數據點的唯一標識符，這裡使用索引值\n",
    "#                 vector=vectors[i],  # 要插入的向量\n",
    "#                 payload=data[i]  # 與該向量相關聯的數據（例如文本）\n",
    "#             )]         \n",
    "#         )  \n",
    "\n",
    "# # 准備將文本塊列表（chunks）轉換為包含文本的字典列表\n",
    "# lst_dic_chunk = [] \n",
    "# for chunk in lst_chunk:     \n",
    "#     lst_dic_chunk.append({\"text\": chunk})  # 將每個文本塊轉換為字典，並添加到列表中\n",
    "\n",
    "# # 連接到 Qdrant 伺服器並創建/重新創建集合\n",
    "# qclient = connection(v_dim=384, collection='重點') \n",
    "\n",
    "# # 將向量和對應的文本數據插入到 Qdrant 集合中\n",
    "# upsert_vector(\n",
    "#     client=qclient, \n",
    "#     collection='重點',  # 指定集合名稱\n",
    "#     vectors=arr_vectors,  # 已經計算好的向量數組\n",
    "#     data=lst_dic_chunk  # 對應的文本數據列表\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.8.0.post1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from faiss-cpu) (23.2)\n",
      "Using cached faiss_cpu-1.8.0.post1-cp312-cp312-win_amd64.whl (14.6 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 -1 -1]] [[0.0000000e+00 3.4028235e+38 3.4028235e+38]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 加载 SentenceTransformer 模型\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# \n",
    "question = '台中捷運公司發行哪四個定期票?'\n",
    "question_emb = model.encode(question)\n",
    "\n",
    "# 創建索引（余弦相似度使用 L2 歸一化）\n",
    "index = faiss.IndexFlatL2(384)\n",
    "faiss.normalize_L2(np.array([question_emb]))\n",
    "\n",
    "# 插入向量\n",
    "index.add(np.array([question_emb]))\n",
    "\n",
    "# 查找向量\n",
    "D, I = index.search(np.array([question_emb]), k=3)\n",
    "print(I, D)  # I 是最近鄰的索引，D 是距離\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最近鄰索引: [[26 78 34]]\n",
      "距離: [[117.33954 118.67166 120.04205]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 初始化 SentenceTransformer 模型，用於將文本轉換為向量\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# 定義要查詢的問題文本\n",
    "question = '台中捷運公司發行哪四個定期票'\n",
    "\n",
    "# 將問題文本編碼為向量表示\n",
    "question_emb = model.encode(question)\n",
    "\n",
    "# 假設你已經有一個存儲向量的矩陣 (例如來自多個文本塊)\n",
    "# 這裡我們用 np.array 創建一個簡單的示例數據集\n",
    "# 如果你有實際的數據集，將這個矩陣替換為實際的向量集\n",
    "stored_vectors = np.random.rand(100, 384).astype('float32')  # 示例: 100 個 384 維向量\n",
    "\n",
    "# 創建 FAISS 索引\n",
    "index = faiss.IndexFlatL2(384)  # 384 是向量的維度，L2 是使用的距離度量\n",
    "\n",
    "# 向索引中添加向量\n",
    "index.add(stored_vectors)\n",
    "\n",
    "# 執行向量搜索\n",
    "# - query_vector: 用於查詢的向量 (question_emb)\n",
    "# - k: 返回的最近鄰數量\n",
    "D, I = index.search(np.array([question_emb], dtype='float32'), k=3)\n",
    "\n",
    "# 打印搜索結果\n",
    "print(\"最近鄰索引:\", I)\n",
    "print(\"距離:\", D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢索答案:行「臺中市定期票」、「彰\n",
    "### 化縣定期票」、「南投縣定期票」及「中彰投苗定期票」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台中捷運公司發行的四個定期票分別為「臺中市定期票」、「彰化縣定期票」、「南投縣定期票」以及「中彰投苗定期票」。\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "import re  \n",
    "\n",
    "# 將 PDF 文件內容分割成多個文本塊的函數\n",
    "def pdf_to_chunk(path, start_page=1, end_page=None): \n",
    "    loader = PyPDFLoader(path) \n",
    "    pages = loader.load() \n",
    "    total_pages = len(pages) \n",
    "    \n",
    "    text_splitter = CharacterTextSplitter( \n",
    "        separator=\"\\n\",  # 使用 \"\\n\" 作為分隔符\n",
    "        chunk_size=256,  # 每個塊的最大字符數為 256\n",
    "        chunk_overlap=20  # 塊之間有 20 個字符的重疊\n",
    "    ) \n",
    "\n",
    "    if end_page is None: \n",
    "        end_page = len(pages) \n",
    "\n",
    "    lst_text = [] \n",
    "    \n",
    "    for i in range(start_page-1, end_page): \n",
    "        chunks = text_splitter.split_text(pages[i].page_content) \n",
    "        for chunk in chunks: \n",
    "            text = re.sub(r'\\s+', ' ', chunk) \n",
    "            lst_text.append(text) \n",
    "    \n",
    "    return lst_text  \n",
    "\n",
    "# Step 1: 分割 PDF 並生成文本塊\n",
    "lst_chunk = pdf_to_chunk('TPASS.pdf')\n",
    "\n",
    "# Step 2: 初始化 SentenceTransformer 模型，用於將文本塊轉換為向量\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# 將文本塊轉換為向量\n",
    "arr_vectors = np.array([model.encode(chunk) for chunk in lst_chunk])\n",
    "\n",
    "# Step 3: 創建 FAISS 索引\n",
    "index = faiss.IndexFlatL2(384)  # 384 是向量的維度，L2 是使用的距離度量\n",
    "\n",
    "# 向索引中添加向量\n",
    "index.add(arr_vectors)\n",
    "\n",
    "# Step 4: 查詢向量資料庫並與 LLM 結合\n",
    "# 定義要查詢的問題文本\n",
    "question = '台中捷運公司發行哪四個定期票?'\n",
    "question_emb = model.encode(question)\n",
    "\n",
    "# 執行向量搜索，找到最相關的文本\n",
    "D, I = index.search(np.array([question_emb]), k=1)  # 找到最相似的1個文本\n",
    "\n",
    "# 根據檢索結果找到對應的文本\n",
    "retrieved_text = lst_chunk[I[0][0]]\n",
    "\n",
    "# 初始化 OpenAI 客戶端\n",
    "client = OpenAI()\n",
    "\n",
    "# 將檢索到的文本與問題一起傳遞給 LLM\n",
    "completion = client.chat.completions.create(   \n",
    "    model=\"gpt-4-turbo\",   \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一位聰明的 AI 助理，使用檢索到的背景資訊回答使用者問題。\"},\n",
    "        {\"role\": \"system\", \"content\": f\"檢索到的背景資訊：{retrieved_text}\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 獲取 LLM 的回答\n",
    "answer = completion.choices[0].message.content\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
